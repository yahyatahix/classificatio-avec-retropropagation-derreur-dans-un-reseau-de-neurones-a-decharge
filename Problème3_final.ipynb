{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEI723, Université de Sherbrooke, Dec. 2020\n",
    "# Problème de classification de caractère manuscrits avec retropropagation de l'erreur dans un réseau se neurones à décharge\n",
    "\n",
    "Ce notebook présente comment l'algorithme de descente de gradient peut être adapté pour la rétropropagation dans un réseau de neurones à décharge avec des fonctions d'activation non différentiables, créé par I. Balafrej, et adapté par A. El Ferdaoussi.\n",
    "\n",
    "Travail inspiré de : \n",
    "1. Surrogate Gradient Learning in Spiking Neural Networks by Zenke & Ganguli (2018) https://arxiv.org/pdf/1901.09948.pdf\n",
    "2. SLAYER: Spike Layer Error Reassignment in Time (2018) https://arxiv.org/pdf/1810.08646.pdf\n",
    "3. Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets (2019) https://arxiv.org/pdf/1901.09049.pdf\n",
    "\n",
    "Ce code a été complété et adapté par \n",
    "\n",
    "* ETTAHI Yahya (CIP:etty2601)\n",
    "* WAHBI Yasmine (CIP:wahy3501)\n",
    "* YARGA Arnaud (CIP:yars2201)\n",
    "\n",
    "Il s'articule suivant le plan suivant :\n",
    "\n",
    "* Importation des données et création du réseau\n",
    "* Entrainement du réseau\n",
    "* Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "yYULU98X49tw"
   },
   "outputs": [],
   "source": [
    "!pip install quantities sparse > /dev/null\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, model_selection, utils\n",
    "import torch\n",
    "import quantities as units\n",
    "from sparse import COO\n",
    "import enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des données et création du réseau\n",
    "On utilise la base de donnée MNIST et on créé un réseau en spécifiant le nombre de couches cachées souhaité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "GVZiHKp65BPp"
   },
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Use the GPU unless there is none available, if you don't have a CUDA enabled GPU, I recommand using Google Colab\n",
    "# available here: https://colab.research.google.com.\n",
    "# Create a new notebook and then go to Runtime -> Change runtime type -> Hardware accelerator -> GPU\n",
    "# This will give you access to a fairly recent GPU for free, for up to 12h continuously\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Let's download the MNIST dataset, available at https://www.openml.org/d/554\n",
    "# You can edit the argument data_home to the directory of your choice.\n",
    "# The dataset will be downloaded there; the default directory is ~/scikit_learn_data/\n",
    "X, y = datasets.fetch_openml('mnist_784', version=1, return_X_y=True, data_home=None)\n",
    "nb_of_samples, nb_of_features = X.shape\n",
    "#X = 70k samples, 28*28 features, y = 70k samples, 1 label (string)\n",
    "\n",
    "# Shuffle the dataset\n",
    "X, y = utils.shuffle(X, y)\n",
    "\n",
    "# Convert the labels (string) to integers for convenience\n",
    "y = np.array(y, dtype=np.int)\n",
    "nb_of_ouputs = np.max(y) + 1\n",
    "\n",
    "# We'll normalize our input data in the range [0., 1[.\n",
    "X = X / pow(2, 8)\n",
    "\n",
    "# And convert the data to a spike train\n",
    "dt = 1*units.ms\n",
    "duration_per_image = 100*units.ms\n",
    "absolute_duration = int(duration_per_image / dt)\n",
    "\n",
    "time_of_spike = (1 - X) * absolute_duration # The brighter the white, the earlier the spike\n",
    "time_of_spike[X < .25] = 0 # \"Remove\" the spikes associated with darker pixels (Presumably less information)\n",
    "\n",
    "sample_id, neuron_idx = np.nonzero(time_of_spike)\n",
    "\n",
    "# We use a sparse COO array to store the spikes for memory requirements\n",
    "# You can use the spike_train variable as if it were a tensor of shape (nb_of_samples, nb_of_features, absolute_duration)\n",
    "spike_train = COO((sample_id, neuron_idx, time_of_spike[sample_id, neuron_idx]),\n",
    "                  np.ones_like(sample_id), shape=(nb_of_samples, nb_of_features, absolute_duration))\n",
    "\n",
    "# We create a network with 'nb_hidden_layer' hidden layers and 1 output layer\n",
    "nb_hidden = 128 # Number of hidden neurons\n",
    "nb_hidden_layer = 1 # Number of hidden layers (at least one)\n",
    "\n",
    "w_array = []\n",
    "\n",
    "w_array.append(torch.empty((nb_of_features, nb_hidden), device=device, dtype=torch.float, requires_grad=True))\n",
    "torch.nn.init.normal_(w_array[-1], mean=0., std=.1)\n",
    "for i in range(nb_hidden_layer-1):\n",
    "  w_array.append(torch.empty((nb_hidden, nb_hidden), device=device, dtype=torch.float, requires_grad=True))\n",
    "  torch.nn.init.normal_(w_array[-1], mean=0., std=.1)\n",
    "w_array.append(torch.empty((nb_hidden, nb_of_ouputs), device=device, dtype=torch.float, requires_grad=True))\n",
    "torch.nn.init.normal_(w_array[-1], mean=0., std=.1)\n",
    "\n",
    "# Split in train/test\n",
    "nb_of_train_samples = int(nb_of_samples * 0.85) # Keep 15% of the dataset for testing\n",
    "train_indices = np.arange(nb_of_train_samples)\n",
    "test_indices = np.arange(nb_of_train_samples, nb_of_samples)\n",
    "\n",
    "class LearningType(enum.Enum):\n",
    "    Classic = 1 # Apprentissage classique\n",
    "    Extrem1 = 2 # La première couche n'est pas incluse dans l'apprentissage\n",
    "    Extrem2 = 3 # Seule la dernière couche est incluse dans l'apprentissage\n",
    "\n",
    "class ActivationFunctionType(enum.Enum):\n",
    "    Relu = 1\n",
    "    Surrogate = 2\n",
    "    Selu = 3\n",
    "    Celu = 4\n",
    "    Tanh = 5\n",
    "    Sigmoid = 6\n",
    "\n",
    "# Relu\n",
    "class SpikeFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0 # We spike when the (potential-threshold) > 0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone() # Clone will create a copy of the numerical value\n",
    "        grad_input[input < 0] = 0 # The derivative of a ReLU function\n",
    "        return grad_input\n",
    "\n",
    "# Surrogate from https://github.com/fzenke/spytorch/blob/master/notebooks/SpyTorchTutorial1.ipynb\n",
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Here we implement our spiking nonlinearity which also implements \n",
    "    the surrogate gradient. By subclassing torch.autograd.Function, \n",
    "    we will be able to use all of PyTorch's autograd functionality.\n",
    "    Here we use the normalized negative part of a fast sigmoid \n",
    "    as this was done in Zenke & Ganguli (2018).\n",
    "    \"\"\"\n",
    "    \n",
    "    scale = 100.0 # controls steepness of surrogate gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we compute a step function of the input Tensor\n",
    "        and return it. ctx is a context object that we use to stash information which \n",
    "        we need to later backpropagate our error signals. To achieve this we use the \n",
    "        ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor we need to compute the \n",
    "        surrogate gradient of the loss with respect to the input. \n",
    "        Here we use the normalized negative part of a fast sigmoid \n",
    "        as this was done in Zenke & Ganguli (2018).\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement\n",
    "On spécifie les paramètres d'entrainement comme le nombre d'ittération, le type d'apprentissage (classique, extrême), et la fonction d'activation. Ensuite on exécute l'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rTAqehCizj65",
    "outputId": "79bb04c2-f87d-4bb7-a797-faecd33b991f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 -- loss : 712.7480\n",
      "Epoch 2 -- loss : 483.9368\n",
      "Epoch 3 -- loss : 300.6449\n",
      "Epoch 4 -- loss : 190.9680\n",
      "Epoch 5 -- loss : 131.5266\n",
      "Epoch 6 -- loss : 116.9042\n",
      "Epoch 7 -- loss : 107.0969\n",
      "Epoch 8 -- loss : 99.5152\n",
      "Epoch 9 -- loss : 93.0759\n",
      "Epoch 10 -- loss : 88.0047\n"
     ]
    }
   ],
   "source": [
    "def run_spiking_layer(input_spike_train, layer_weights, tau_v=20*units.ms, tau_i=5*units.ms, v_threshold=1.0):\n",
    "    \"\"\"Here we implement a current-LIF dynamic in pytorch\"\"\"\n",
    "\n",
    "    # First, we multiply the input spike train by the weights of the current layer to get the current that will be added\n",
    "    # We can calculate this beforehand because the weights are constant in the forward pass (no plasticity)\n",
    "    input_current = torch.einsum(\"abc,bd->adc\", (input_spike_train, layer_weights)) # Equivalent to a matrix multiplication for tensors of dim > 2 using Einstein's Notation\n",
    "\n",
    "    recorded_spikes = [] # Array of the output spikes at each time t\n",
    "    membrane_potential_at_t = torch.zeros((input_spike_train.shape[0], layer_weights.shape[-1]), device=device, dtype=torch.float)\n",
    "    membrane_current_at_t = torch.zeros((input_spike_train.shape[0], layer_weights.shape[-1]), device=device, dtype=torch.float)\n",
    "\n",
    "    for t in range(absolute_duration): # For every timestep\n",
    "        # Apply the leak\n",
    "        membrane_potential_at_t = float(np.exp(-dt/tau_v))*membrane_potential_at_t # Using tau_v with euler or exact method\n",
    "        membrane_current_at_t = float(np.exp(-dt/tau_i))*membrane_current_at_t # Using tau_i with euler or exact method\n",
    "\n",
    "        # Select the input current at time t\n",
    "        input_at_t = input_current[:, :, t]\n",
    "\n",
    "        # Integrate the input current\n",
    "        membrane_current_at_t += input_at_t\n",
    "\n",
    "        # Integrate the input to the membrane potential\n",
    "        membrane_potential_at_t += membrane_current_at_t\n",
    "\n",
    "        # Apply the non-differentiable function\n",
    "        if activationFunctionType == ActivationFunctionType.Surrogate:\n",
    "            recorded_spikes_at_t = SurrGradSpike.apply(membrane_potential_at_t - v_threshold)\n",
    "        elif activationFunctionType == ActivationFunctionType.Selu:\n",
    "            # From https://pytorch.org/docs/stable/generated/torch.nn.SELU.html#torch.nn.SELU\n",
    "            mSelu = torch.nn.SELU()\n",
    "            recorded_spikes_at_t = mSELU(membrane_potential_at_t - v_threshold)\n",
    "        elif activationFunctionType == ActivationFunctionType.Celu:\n",
    "            # From https://pytorch.org/docs/stable/generated/torch.nn.CELU.html#torch.nn.CELU\n",
    "            mCelu = torch.nn.CELU()\n",
    "            recorded_spikes_at_t = mCelu(membrane_potential_at_t - v_threshold)\n",
    "        elif activationFunctionType == ActivationFunctionType.Tanh:\n",
    "            # From https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh\n",
    "            mTanh = torch.nn.Tanh()\n",
    "            recorded_spikes_at_t = mTanh(membrane_potential_at_t - v_threshold)\n",
    "        elif activationFunctionType == ActivationFunctionType.Sigmoid:\n",
    "            # From https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid\n",
    "            mSig = torch.nn.Sigmoid()\n",
    "            recorded_spikes_at_t = mSig(membrane_potential_at_t - v_threshold)\n",
    "        else:\n",
    "            recorded_spikes_at_t = SpikeFunction.apply(membrane_potential_at_t - v_threshold)\n",
    "        recorded_spikes.append(recorded_spikes_at_t)\n",
    "\n",
    "        # Reset the spiked neurons\n",
    "        membrane_potential_at_t[membrane_potential_at_t > v_threshold] = 0\n",
    "\n",
    "    recorded_spikes = torch.stack(recorded_spikes, dim=2) # Stack over time axis (Array -> Tensor)\n",
    "    return recorded_spikes\n",
    "\n",
    "\n",
    "# Set-up training\n",
    "nb_of_epochs = 10\n",
    "batch_size = 256 # The backpropagation is done after every batch, but a batch here is also used for memory requirements \n",
    "number_of_batches = len(train_indices) // batch_size\n",
    "# Choose the Activation function\n",
    "activationFunctionType = ActivationFunctionType.Sigmoid\n",
    "# We choose the type of learning to be used\n",
    "learningType = LearningType.Classic\n",
    "\n",
    "# Trainable parameters depends on learning type\n",
    "if (learningType==LearningType.Extrem1):\n",
    "    params = w_array[1:-1]\n",
    "elif (learningType==LearningType.Extrem2):\n",
    "    params = [w_array[-1]]\n",
    "else:\n",
    "    params = w_array\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=0.01, amsgrad=True)\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "for e in range(nb_of_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in np.array_split(train_indices, number_of_batches):\n",
    "        # Select batch and convert to tensors\n",
    "        batch_spike_train = torch.FloatTensor(spike_train[batch].todense()).to(device)\n",
    "        batch_labels = torch.LongTensor(y[batch, np.newaxis]).to(device)\n",
    "\n",
    "        # Here we create a target spike count (10 spikes for wrong label, 100 spikes for true label) in a one-hot fashion\n",
    "        # This approach is seen in Shrestha & Orchard (2018) https://arxiv.org/pdf/1810.08646.pdf\n",
    "        # Code available at https://github.com/bamsumit/slayerPytorch\n",
    "        min_spike_count = 10 * torch.ones((batch.shape[0], 10), device=device, dtype=torch.float)\n",
    "        target_output = min_spike_count.scatter_(1, batch_labels, 100.0)\n",
    "\n",
    "        # Forward propagation\n",
    "        layer_1_spikes = run_spiking_layer(batch_spike_train, w_array[0])\n",
    "        layer_spikes_array = [layer_1_spikes]\n",
    "        for i in range(nb_hidden_layer-1):\n",
    "          layer_spikes_array.append(run_spiking_layer(layer_spikes_array[-1], w_array[i+1]))\n",
    "        layer_last_spikes = run_spiking_layer(layer_spikes_array[-1], w_array[-1])\n",
    "        network_output = torch.sum(layer_last_spikes, 2) # Count the spikes over time axis\n",
    "        loss = loss_fn(network_output, target_output)\n",
    "\n",
    "        # Backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(\"Epoch %i -- loss : %.4f\" %(e+1, epoch_loss / number_of_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "On évalue la précision du réseau entrainé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wZ4lXC1u9OOT",
    "outputId": "ecc06220-7743-40f4-dc05-e2aa79d46d9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on test set: 0.966\n"
     ]
    }
   ],
   "source": [
    "# Test the accuracy of the model\n",
    "correct_label_count = 0\n",
    "# We only need to batchify the test set for memory requirements\n",
    "for batch in np.array_split(test_indices,  len(test_indices) // batch_size):\n",
    "    test_spike_train = torch.FloatTensor(spike_train[batch].todense()).to(device)\n",
    "  \n",
    "    # Same forward propagation as before\n",
    "    layer_1_spikes = run_spiking_layer(test_spike_train, w_array[0])\n",
    "    layer_spikes_array = [layer_1_spikes]\n",
    "    for i in range(nb_hidden_layer-1):\n",
    "      layer_spikes_array.append(run_spiking_layer(layer_spikes_array[-1], w_array[i+1]))\n",
    "    layer_last_spikes = run_spiking_layer(layer_spikes_array[-1], w_array[-1])\n",
    "    network_output = torch.sum(layer_last_spikes, 2) # Count the spikes over time axis\n",
    "    \n",
    "    # Do the prediction by selecting the output neuron with the most number of spikes\n",
    "    _, am = torch.max(network_output, 1) \n",
    "    correct_label_count += np.sum(am.detach().cpu().numpy() == y[batch])\n",
    "\n",
    "print(\"Model accuracy on test set: %.3f\" % (correct_label_count / len(test_indices)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Problème3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
